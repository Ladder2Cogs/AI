{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing Ineuality\n",
    "\n",
    "For a set of three random variables satisfying the dependency\n",
    "$$X \\rightarrow Y \\rightarrow Z$$\n",
    "we have that\n",
    "$$I(X;Z) \\leq I(X;Y)$$\n",
    "where $I$ is the mutual information\n",
    "\n",
    "For a deep neural network\n",
    "$$Input \\rightarrow layer_{1} \\rightarrow layer_{2} \\rightarrow ... \\rightarrow layer_{n} \\rightarrow Output$$ \n",
    "while every layer has less infomation than its parent layer, it can distill computationally useful representations layer by layer, leading to desired input-output behavior in the final layer ([Alain & Bengio, 2016](https://arxiv.org/pdf/1610.01644)).\n",
    "\n",
    "#### Probing\n",
    "\n",
    "The *probing* technique reveals how raw input is gradually transformed into task-relevant representations. For raw input $X$, we collect the feature vectors $H_k$ from intermediate layer $k$. A *probe* model $f_k$ is employed to examine whether specific information (labels $y$) is represented by $H_k$:\n",
    "$$f_k: H_k \\rightarrow y$$\n",
    "\n",
    "Notes about probing from [Christopher Potts](https://www.youtube.com/watch?v=ElDtkhqv5ZE&t=623s)\n",
    "\n",
    "**Core idea:** use supervised models (probes) to determine what is latently encoded in the hidden representations of our target models.\n",
    "\n",
    "**Caution:** (1) a very powerful probe might lead you to see things that aren't in the target model, but rather in the probe; (2) probes can not tell us about whether the information that we identify has any causal relationship with the target model's input-output behavior.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
